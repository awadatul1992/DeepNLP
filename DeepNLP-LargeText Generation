{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepNLP-LargeText Generation","provenance":[],"authorship_tag":"ABX9TyMnLlPq5WD3Vmqyo0T1DD4L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2Eyo3z7Jrw30","executionInfo":{"status":"ok","timestamp":1601464199879,"user_tz":-330,"elapsed":946,"user":{"displayName":"atul awad","photoUrl":"","userId":"09165659967253375925"}},"outputId":"a58fcba2-72f8-4817-87f0-46d551e731fb","colab":{"base_uri":"https://localhost:8080/","height":193}},"source":["import string\n","import re\n","# load doc into memory\n","def load_doc(filename):\n","  # open the file as read only\n","  file = open(filename, 'r')\n","  # read all text\n","  text = file.read()\n","  # close the file\n","  file.close()\n","  return text\n","# turn a doc into clean tokens\n","def clean_doc(doc):\n","  # replace '--' with a space ' '\n","  doc = doc.replace('--', ' ')\n","  # split into tokens by white space\n","  tokens = doc.split()\n","  # prepare regex for char filtering\n","  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n","  # remove punctuation from each word\n","  tokens = [re_punc.sub('', w) for w in tokens]\n","  # remove remaining tokens that are not alphabetic\n","  tokens = [word for word in tokens if word.isalpha()]\n","  # make lower case\n","  tokens = [word.lower() for word in tokens]\n","  return tokens\n","# save tokens to file, one dialog per line\n","def save_doc(lines, filename):\n","  data = '\\n'.join(lines)\n","  file = open(filename, 'w')\n","  file.write(data)\n","  file.close()\n","# load document\n","in_filename = '/content/republic_clean.txt'\n","doc = load_doc(in_filename)\n","print(doc[:200])\n","# clean document\n","tokens = clean_doc(doc)\n","print(tokens[:200])\n","print('Total Tokens: %d' % len(tokens))\n","print('Unique Tokens: %d' % len(set(tokens)))\n","# organize into sequences of tokens\n","length = 50 + 1\n","sequences = list()\n","for i in range(length, len(tokens)):\n","  # select sequence of tokens\n","  seq = tokens[i-length:i]\n","  # convert into a line\n","  line = ' '.join(seq)\n","  # store\n","  sequences.append(line)\n","print('Total Sequences: %d' % len(sequences))\n","# save sequences to file\n","out_filename = '/content/republic_sequences.txt'\n","save_doc(sequences, out_filename)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["BOOK I.\n","\n","I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n","that I might offer up my prayers to the goddess (Bendis, the Thracian\n","Artemis.); and also because I wanted to see in what\n","['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n","Total Tokens: 118684\n","Unique Tokens: 7409\n","Total Sequences: 118633\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XHhOzdeQscdq","executionInfo":{"status":"ok","timestamp":1601465794631,"user_tz":-330,"elapsed":1463644,"user":{"displayName":"atul awad","photoUrl":"","userId":"09165659967253375925"}},"outputId":"89af840d-5053-492f-e099-8a09c89f4e0e","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from numpy import array\n","from pickle import dump\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils.vis_utils import plot_model\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","# load doc into memory\n","def load_doc(filename):\n","  # open the file as read only\n","  file = open(filename, 'r')\n","  # read all text\n","  text = file.read()\n","  # close the file\n","  file.close()\n","  return text\n","# define the model\n","def define_model(vocab_size, seq_length):\n","  model = Sequential()\n","  model.add(Embedding(vocab_size, 50, input_length=seq_length))\n","  model.add(LSTM(100, return_sequences=True))\n","  model.add(LSTM(100))\n","  model.add(Dense(100, activation='relu'))\n","  model.add(Dense(vocab_size, activation='softmax'))\n","  # compile network\n","  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","  # summarize defined model\n","  model.summary()\n","  plot_model(model, to_file='model.png', show_shapes=True)\n","  return model\n","# load\n","in_filename = '/content/republic_sequences.txt'\n","doc = load_doc(in_filename)\n","lines = doc.split('\\n')\n","# integer encode sequences of words\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(lines)\n","sequences = tokenizer.texts_to_sequences(lines)\n","# vocabulary size\n","vocab_size = len(tokenizer.word_index) + 1\n","# separate into input and output\n","sequences = array(sequences)\n","X, y = sequences[:,:-1], sequences[:,-1]\n","y = to_categorical(y, num_classes=vocab_size)\n","seq_length = X.shape[1]\n","# define model\n","model = define_model(vocab_size, seq_length)\n","# fit model\n","model.fit(X, y, batch_size=128, epochs=100)\n","# save the model to file\n","model.save('/content/model.h5')\n","# save the tokenizer\n","dump(tokenizer, open('/content/tokenizer.pkl', 'wb'))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 50, 50)            370500    \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 50, 100)           60400     \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 100)               80400     \n","_________________________________________________________________\n","dense (Dense)                (None, 100)               10100     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 7410)              748410    \n","=================================================================\n","Total params: 1,269,810\n","Trainable params: 1,269,810\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","927/927 [==============================] - 15s 17ms/step - loss: 6.1694 - accuracy: 0.0716\n","Epoch 2/100\n","927/927 [==============================] - 15s 16ms/step - loss: 5.6916 - accuracy: 0.1071\n","Epoch 3/100\n","927/927 [==============================] - 15s 16ms/step - loss: 5.4650 - accuracy: 0.1269\n","Epoch 4/100\n","927/927 [==============================] - 14s 15ms/step - loss: 5.3025 - accuracy: 0.1425\n","Epoch 5/100\n","927/927 [==============================] - 14s 15ms/step - loss: 5.1701 - accuracy: 0.1533\n","Epoch 6/100\n","927/927 [==============================] - 15s 16ms/step - loss: 5.0593 - accuracy: 0.1623\n","Epoch 7/100\n","927/927 [==============================] - 14s 15ms/step - loss: 4.9624 - accuracy: 0.1689\n","Epoch 8/100\n","927/927 [==============================] - 14s 15ms/step - loss: 4.8706 - accuracy: 0.1740\n","Epoch 9/100\n","927/927 [==============================] - 15s 16ms/step - loss: 4.8051 - accuracy: 0.1759\n","Epoch 10/100\n","927/927 [==============================] - 15s 16ms/step - loss: 4.7008 - accuracy: 0.1829\n","Epoch 11/100\n","927/927 [==============================] - 15s 16ms/step - loss: 4.6190 - accuracy: 0.1862\n","Epoch 12/100\n","927/927 [==============================] - 15s 16ms/step - loss: 4.5404 - accuracy: 0.1893\n","Epoch 13/100\n","927/927 [==============================] - 14s 15ms/step - loss: 4.4650 - accuracy: 0.1926\n","Epoch 14/100\n","927/927 [==============================] - 14s 15ms/step - loss: 4.3918 - accuracy: 0.1955\n","Epoch 15/100\n","927/927 [==============================] - 14s 15ms/step - loss: 4.3209 - accuracy: 0.1979\n","Epoch 16/100\n","927/927 [==============================] - 14s 15ms/step - loss: 4.2524 - accuracy: 0.2010\n","Epoch 17/100\n","927/927 [==============================] - 14s 16ms/step - loss: 4.1879 - accuracy: 0.2043\n","Epoch 18/100\n","927/927 [==============================] - 14s 15ms/step - loss: 4.1254 - accuracy: 0.2077\n","Epoch 19/100\n","927/927 [==============================] - 14s 15ms/step - loss: 4.0660 - accuracy: 0.2113\n","Epoch 20/100\n","927/927 [==============================] - 15s 16ms/step - loss: 4.0082 - accuracy: 0.2155\n","Epoch 21/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.9552 - accuracy: 0.2197\n","Epoch 22/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.9043 - accuracy: 0.2233\n","Epoch 23/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.8570 - accuracy: 0.2287\n","Epoch 24/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.8116 - accuracy: 0.2327\n","Epoch 25/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.7701 - accuracy: 0.2364\n","Epoch 26/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.7270 - accuracy: 0.2413\n","Epoch 27/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.6875 - accuracy: 0.2453\n","Epoch 28/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.6523 - accuracy: 0.2502\n","Epoch 29/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.6175 - accuracy: 0.2535\n","Epoch 30/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.5787 - accuracy: 0.2572\n","Epoch 31/100\n","927/927 [==============================] - 15s 16ms/step - loss: 3.5401 - accuracy: 0.2628\n","Epoch 32/100\n","927/927 [==============================] - 14s 16ms/step - loss: 3.5074 - accuracy: 0.2659\n","Epoch 33/100\n","927/927 [==============================] - 14s 16ms/step - loss: 3.4743 - accuracy: 0.2705\n","Epoch 34/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.4391 - accuracy: 0.2750\n","Epoch 35/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.4073 - accuracy: 0.2790\n","Epoch 36/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.3765 - accuracy: 0.2828\n","Epoch 37/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.3456 - accuracy: 0.2883\n","Epoch 38/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.3144 - accuracy: 0.2911\n","Epoch 39/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.2857 - accuracy: 0.2959\n","Epoch 40/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.2557 - accuracy: 0.3002\n","Epoch 41/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.2275 - accuracy: 0.3046\n","Epoch 42/100\n","927/927 [==============================] - 15s 16ms/step - loss: 3.1977 - accuracy: 0.3081\n","Epoch 43/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.1704 - accuracy: 0.3136\n","Epoch 44/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.1435 - accuracy: 0.3164\n","Epoch 45/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.1152 - accuracy: 0.3206\n","Epoch 46/100\n","927/927 [==============================] - 15s 16ms/step - loss: 3.0907 - accuracy: 0.3240\n","Epoch 47/100\n","927/927 [==============================] - 14s 15ms/step - loss: 3.0640 - accuracy: 0.3295\n","Epoch 48/100\n","927/927 [==============================] - 14s 16ms/step - loss: 3.0375 - accuracy: 0.3332\n","Epoch 49/100\n","927/927 [==============================] - 15s 16ms/step - loss: 3.0128 - accuracy: 0.3376\n","Epoch 50/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.9879 - accuracy: 0.3408\n","Epoch 51/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.9629 - accuracy: 0.3438\n","Epoch 52/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.9401 - accuracy: 0.3475\n","Epoch 53/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.9170 - accuracy: 0.3522\n","Epoch 54/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.8918 - accuracy: 0.3567\n","Epoch 55/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.8685 - accuracy: 0.3587\n","Epoch 56/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.8480 - accuracy: 0.3628\n","Epoch 57/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.8225 - accuracy: 0.3673\n","Epoch 58/100\n","927/927 [==============================] - 15s 16ms/step - loss: 2.8022 - accuracy: 0.3700\n","Epoch 59/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.7804 - accuracy: 0.3742\n","Epoch 60/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.7598 - accuracy: 0.3779\n","Epoch 61/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.7373 - accuracy: 0.3808\n","Epoch 62/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.7166 - accuracy: 0.3853\n","Epoch 63/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.6966 - accuracy: 0.3878\n","Epoch 64/100\n","927/927 [==============================] - 15s 16ms/step - loss: 2.6761 - accuracy: 0.3915\n","Epoch 65/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.6563 - accuracy: 0.3956\n","Epoch 66/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.6352 - accuracy: 0.3969\n","Epoch 67/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.6166 - accuracy: 0.4013\n","Epoch 68/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.5964 - accuracy: 0.4044\n","Epoch 69/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.5784 - accuracy: 0.4085\n","Epoch 70/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.5579 - accuracy: 0.4102\n","Epoch 71/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.5404 - accuracy: 0.4149\n","Epoch 72/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.5230 - accuracy: 0.4191\n","Epoch 73/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.5033 - accuracy: 0.4215\n","Epoch 74/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.4875 - accuracy: 0.4237\n","Epoch 75/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.4682 - accuracy: 0.4280\n","Epoch 76/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.4515 - accuracy: 0.4301\n","Epoch 77/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.4343 - accuracy: 0.4343\n","Epoch 78/100\n","927/927 [==============================] - 15s 16ms/step - loss: 2.4197 - accuracy: 0.4365\n","Epoch 79/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.3982 - accuracy: 0.4393\n","Epoch 80/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.3842 - accuracy: 0.4439\n","Epoch 81/100\n","927/927 [==============================] - 15s 16ms/step - loss: 2.3683 - accuracy: 0.4456\n","Epoch 82/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.3518 - accuracy: 0.4493\n","Epoch 83/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.3351 - accuracy: 0.4517\n","Epoch 84/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.3212 - accuracy: 0.4539\n","Epoch 85/100\n","927/927 [==============================] - 15s 16ms/step - loss: 2.3025 - accuracy: 0.4577\n","Epoch 86/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.2870 - accuracy: 0.4601\n","Epoch 87/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.2706 - accuracy: 0.4642\n","Epoch 88/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.2591 - accuracy: 0.4660\n","Epoch 89/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.2418 - accuracy: 0.4705\n","Epoch 90/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.2260 - accuracy: 0.4719\n","Epoch 91/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.2142 - accuracy: 0.4735\n","Epoch 92/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.1956 - accuracy: 0.4785\n","Epoch 93/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.1857 - accuracy: 0.4790\n","Epoch 94/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.1680 - accuracy: 0.4834\n","Epoch 95/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.1544 - accuracy: 0.4857\n","Epoch 96/100\n","927/927 [==============================] - 15s 16ms/step - loss: 2.1398 - accuracy: 0.4873\n","Epoch 97/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.1266 - accuracy: 0.4911\n","Epoch 98/100\n","927/927 [==============================] - 14s 15ms/step - loss: 2.1129 - accuracy: 0.4937\n","Epoch 99/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.1019 - accuracy: 0.4958\n","Epoch 100/100\n","927/927 [==============================] - 14s 16ms/step - loss: 2.0877 - accuracy: 0.5003\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UBrGnhx8tmqh","executionInfo":{"status":"ok","timestamp":1601466089949,"user_tz":-330,"elapsed":3584,"user":{"displayName":"atul awad","photoUrl":"","userId":"09165659967253375925"}},"outputId":"3afd1848-2a9b-41d9-a822-fe011c7f4216","colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["from random import randint\n","from pickle import load\n","from keras.models import load_model\n","from keras.preprocessing.sequence import pad_sequences\n","# load doc into memory\n","def load_doc(filename):\n","  # open the file as read only\n","  file = open(filename, 'r')\n","  # read all text\n","  text = file.read()\n","  # close the file\n","  file.close()\n","  return text\n","# generate a sequence from a language model\n","def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n","  result = list()\n","  in_text = seed_text\n","  # generate a fixed number of words\n","  for _ in range(n_words):\n","    # encode the text as integer\n","    encoded = tokenizer.texts_to_sequences([in_text])[0]\n","    # truncate sequences to a fixed length\n","    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n","    # predict probabilities for each word\n","    yhat = model.predict_classes(encoded, verbose=0)\n","    # map predicted word index to word\n","    out_word = ''\n","    for word, index in tokenizer.word_index.items():\n","      if index == yhat:\n","        out_word = word\n","        break\n","    # append to input\n","    in_text += ' ' + out_word\n","    result.append(out_word)\n","  return ' '.join(result)\n","# load cleaned text sequences\n","in_filename = 'republic_sequences.txt'\n","doc = load_doc(in_filename)\n","lines = doc.split('\\n')\n","seq_length = len(lines[0].split()) - 1\n","# load the model\n","model = load_model('model.h5')\n","# load the tokenizer\n","tokenizer = load(open('tokenizer.pkl', 'rb'))\n","# select a seed text\n","seed_text = lines[randint(0,len(lines))]\n","print(seed_text + '\\n')\n","# generate new text\n","generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n","print(generated)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["perfectly unjust state will be most likely to do so i know i said that such was your position but what i would further consider is whether this power which is possessed by the superior state can exist or be exercised without justice or only with justice if you are right\n","\n","he replied and i should like to know what you mean i said that the rulers in words is of the body youth and the other of the rich and the other is the only ones and the most pugnacious of government is not the cause of the gods certainly\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"meIsVRJEzzjA"},"source":[""],"execution_count":null,"outputs":[]}]}